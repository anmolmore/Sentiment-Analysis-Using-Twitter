{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tweepy.api.API object at 0x10d398a58>\n"
     ]
    }
   ],
   "source": [
    "import tweepy\n",
    "from TwitterAPI import TwitterAPI\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import timedelta, date\n",
    "import time\n",
    "\n",
    "consumer_key = \"fBKRihVu5bNj0ypeey39J8v6x\"\n",
    "consumer_secret = \"E4fkBQMCfO9WGg6ZDitWxFRAdWrjIPaL24RTNSpnTXIsfGdO6g\"\n",
    "\n",
    "access_token = \"1146388838534660096-iYJJ8NXMf16sNj0O6jd6HzsjvtjfVp\"\n",
    "access_token_secret = \"r16edV26xmYuxj1wyzOhgZnvqKcYjnXqxaMRcMv8nUk75\"\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "print(api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "#for each response, filter out important attributes for dataframe\n",
    "def process_tweet(tweet) :\n",
    "    tweet_date = tweet['created_at']\n",
    "    ts = time.strftime('%Y-%m-%d %H:%M:%S', time.strptime(tweet_date,'%a %b %d %H:%M:%S +0000 %Y'))\n",
    "\n",
    "    tweet_id = tweet['id']\n",
    "    tweet_text = tweet['text']\n",
    "    if('extended_tweet' in tweet.keys()):\n",
    "        tweet_text = tweet['extended_tweet']['full_text']\n",
    "\n",
    "    user_id = tweet['user']['id']\n",
    "    followers_count = tweet['user']['followers_count']\n",
    "    friends_count = tweet['user']['friends_count']\n",
    "\n",
    "    user_mentions = tweet['entities']['user_mentions']\n",
    "    screen_names = [user_mention['screen_name'] for user_mention in user_mentions]\n",
    "    screen_name = tweet['user']['screen_name']\n",
    "    retweet_count = tweet['retweet_count']\n",
    "    favorite_count = tweet['favorite_count']\n",
    "    retweeted = tweet['retweeted']\n",
    "    tweet_row = {'date':ts,\n",
    "                 'tweet_id' : tweet_id,\n",
    "                 'user_id' : user_id,\n",
    "                 'followers_count' : followers_count,\n",
    "                 'friends_count' : friends_count,\n",
    "                 'user_mentions' : screen_names,\n",
    "                 'screen_name' : screen_name,\n",
    "                 'retweet_count' : retweet_count,\n",
    "                 'favorite_count' : favorite_count,\n",
    "                 'retweeted' : retweeted,\n",
    "                 'full_text':tweet_text}\n",
    "    return tweet_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ref : https://stackoverflow.com/questions/1060279/iterating-through-a-range-of-dates-in-python\n",
    "def daterange(start_date, end_date):\n",
    "    for n in range(int ((end_date - start_date).days)):\n",
    "        yield start_date + timedelta(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201906200000\n",
      "201906202359\n",
      "{}\n",
      "web_request_count:  1\n",
      "status_code:  200\n",
      "eyJhdXRoZW50aWNpdHkiOiIzNWZiMjQ4MTAzY2M5NmMxN2NiZDYyNWY3M2RiMDYxYWIwZDJiOGI1MDRlNzFlNjQ5NTY4YTJmYjU3MjA2OGM5IiwiZnJvbURhdGUiOiIyMDE5MDYyMDAwMDAiLCJ0b0RhdGUiOiIyMDE5MDYyMDIzNTkiLCJuZXh0IjoiMjAxOTA2MjAyMzU5MDAtMTE0MTc1NjgwNDk3MzUxODg0OC0wIn0=\n",
      "web_request_count:  2\n",
      "status_code:  200\n",
      "eyJhdXRoZW50aWNpdHkiOiI4NmZiYWQ1MDAwMDhhMzgwY2E4ZGQxOTJiZGNmOTAwYjQ5M2RhOTQ1OTNkYjk0OGE5NzM4ZTk0Nzg3NjQ1MTg3IiwiZnJvbURhdGUiOiIyMDE5MDYyMDAwMDAiLCJ0b0RhdGUiOiIyMDE5MDYyMDIzNTkiLCJuZXh0IjoiMjAxOTA2MjAyMzU5MDAtMTE0MTczMzkxNzM1Mjk2NDA5Ni0wIn0=\n",
      "web_request_count:  3\n",
      "status_code:  200\n",
      "eyJhdXRoZW50aWNpdHkiOiIwM2U5Y2ZlMzAwMGEzYTQxNzc1NDg0YTE2NzVkYjI5MGNlMWRmYWVkM2IxNjUzN2ViMjYxMmQ4YmI4MTg1NDY2IiwiZnJvbURhdGUiOiIyMDE5MDYyMDAwMDAiLCJ0b0RhdGUiOiIyMDE5MDYyMDIzNTkiLCJuZXh0IjoiMjAxOTA2MjAyMzU5MDAtMTE0MTcxODc1MzUxNzc4OTE4NC0wIn0=\n",
      "web_request_count:  4\n",
      "status_code:  200\n",
      "eyJhdXRoZW50aWNpdHkiOiJjMGQ4OTE1ZTljMmY1ZGIyZmJjMmViYjQ4MTNjOTU0YzIzZTkyODAwY2FkMGY0ZDUzNjYyNDY0MmUwYjZjMmM5IiwiZnJvbURhdGUiOiIyMDE5MDYyMDAwMDAiLCJ0b0RhdGUiOiIyMDE5MDYyMDIzNTkiLCJuZXh0IjoiMjAxOTA2MjAyMzU5MDAtMTE0MTY3NjAzMzAwMDY2OTE4NS0wIn0=\n",
      "web_request_count:  5\n",
      "status_code:  200\n",
      "(497, 11)\n"
     ]
    }
   ],
   "source": [
    "#Ref : https://medium.com/@poconnell732/acquiring-free-historical-geo-located-data-from-twitter-1f8f2821e9b1\n",
    "api = TwitterAPI(consumer_key=consumer_key,\n",
    "                 consumer_secret=consumer_secret,\n",
    "                 access_token_key=access_token,\n",
    "                 access_token_secret=access_token_secret)\n",
    "PRODUCT = '30day' #Using 30day API\n",
    "LABEL = '30daysdev' # sandbox name\n",
    "web_request_count = 0\n",
    "                \n",
    "start_month_date = date(2019, 6, 20)\n",
    "end_month_date = date(2019, 6, 21)\n",
    "\n",
    "for date in daterange(start_month_date, end_month_date):\n",
    "    list_tweets = []\n",
    "    next_token = {}\n",
    "    date_str = date.strftime(\"%Y%m%d\")\n",
    "    start_date = date_str + \"0000\"\n",
    "    end_date = date_str + \"2359\"\n",
    "    print(start_date)\n",
    "    print(end_date)\n",
    "    \n",
    "    while ((next_token is not None) and (web_request_count<5)):\n",
    "        print(next_token)\n",
    "        if not next_token :\n",
    "            req_dict = {'query' : '@swiggy_in OR @swiggyCares lang:en', 'fromDate': start_date, 'toDate': end_date,}\n",
    "        else :\n",
    "            req_dict['next'] = next_token\n",
    "        \n",
    "        web_request_count += 1\n",
    "        print('web_request_count: ', web_request_count)\n",
    "        req = api.request('tweets/search/%s/:%s' % (PRODUCT, LABEL), req_dict)\n",
    "        print('status_code: ', req.status_code)\n",
    "        response = req.json()\n",
    "        if('next' in response.keys()):\n",
    "            next_token = response['next']\n",
    "        else :\n",
    "            next_token = None\n",
    "        \n",
    "        #print(response)\n",
    "        results = response['results']\n",
    "        with open('swiggy/json/' + date_str + '_' + str(web_request_count) + '.json', 'w+') as f:\n",
    "            json.dump(results, f)\n",
    "        \n",
    "        #print(results)\n",
    "        for tweet in results:\n",
    "            tweet_row = process_tweet(tweet)\n",
    "            list_tweets.append(tweet_row)\n",
    "        df = pd.DataFrame(list_tweets)\n",
    "    df.to_csv('swiggy/' + date_str + '.csv', index=False)\n",
    "    print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ref : https://medium.com/@poconnell732/acquiring-free-historical-geo-located-data-from-twitter-1f8f2821e9b1\n",
    "from TwitterAPI import TwitterAPI\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import timedelta, date\n",
    "\n",
    "api = TwitterAPI(consumer_key=consumer_key,\n",
    "                 consumer_secret=consumer_secret,\n",
    "                 access_token_key=access_token,\n",
    "                 access_token_secret=access_token_secret)\n",
    "PRODUCT = '30day' #Using 30day API\n",
    "LABEL = '30daysdev' # sandbox name\n",
    "web_request_count = 0\n",
    "                \n",
    "start_month_date = date(2019, 6, 13)\n",
    "end_month_date = date(2019, 6, 13)\n",
    "\n",
    "for date in daterange(start_month_date, end_month_date):\n",
    "    list_tweets = []\n",
    "    next_token = {}\n",
    "    date_str = date.strftime(\"%Y%m%d\")\n",
    "    start_date = date_str + \"0000\"\n",
    "    end_date = date_str + \"2359\"\n",
    "    print(start_date)\n",
    "    print(end_date)\n",
    "    \n",
    "    while (next_token is not None) and (web_request_count<10):\n",
    "        print(next_token)\n",
    "        if not next_token :\n",
    "            req_dict = {'query' : '@zomatocare OR @zomatoIn lang:en', 'fromDate': start_date, 'toDate': end_date}\n",
    "        else :\n",
    "            req_dict['next'] = next_token\n",
    "        \n",
    "        web_request_count += 1\n",
    "        print('web_request_count: ', web_request_count)\n",
    "        req = api.request('tweets/search/%s/:%s' % (PRODUCT, LABEL), req_dict)\n",
    "        print('status_code: ', req.status_code)\n",
    "        response = req.json()\n",
    "        if('next' in response.keys()):\n",
    "            next_token = response['next']\n",
    "        else :\n",
    "            next_token = None\n",
    "        \n",
    "        results = response['results']\n",
    "        with open('zomato/json/' + date_str + '_' + str(web_request_count) + '.json', 'w+') as f:\n",
    "            json.dump(results, f)\n",
    "        \n",
    "        #print(results)\n",
    "        for tweet in results:\n",
    "            tweet_row = process_tweet(tweet)\n",
    "            list_tweets.append(tweet_row)\n",
    "        df = pd.DataFrame(list_tweets)\n",
    "    df.to_csv('zomato/' + date_str + '.csv', index=False)\n",
    "    print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
