{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from TwitterAPI import TwitterAPI\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import timedelta, date\n",
    "import time\n",
    "\n",
    "#vkumar.0101@gmail.com\n",
    "consumer_key = \"eQ8ErZL3vEvZkQvTSKMPndhzb\"\n",
    "consumer_secret = \"ztZJo14QU1wFtREneeQ7HhBUbK2RidqIIHCvXKSZDjEqVMTprv\"\n",
    "\n",
    "access_token = \"115683208-cdY9Yru9aaStNHr9QtA2uLTn4khAqpoL33HvPKgm\"\n",
    "access_token_secret = \"4vheUcufR5nPw9WAYbMs0omnjc97KmYaN2fk3dYQ0O1UO\"\n",
    "\n",
    "#moreanmol@gmail.com\n",
    "# consumer_key = \"fBKRihVu5bNj0ypeey39J8v6x\"\n",
    "# consumer_secret = \"E4fkBQMCfO9WGg6ZDitWxFRAdWrjIPaL24RTNSpnTXIsfGdO6g\"\n",
    "\n",
    "# access_token = \"1146388838534660096-iYJJ8NXMf16sNj0O6jd6HzsjvtjfVp\"\n",
    "# access_token_secret = \"r16edV26xmYuxj1wyzOhgZnvqKcYjnXqxaMRcMv8nUk75\"\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "print(api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "#for each response, filter out important attributes for dataframe\n",
    "def process_tweet(tweet) :\n",
    "    tweet_date = tweet['created_at']\n",
    "    ts = time.strftime('%Y-%m-%d %H:%M:%S', time.strptime(tweet_date,'%a %b %d %H:%M:%S +0000 %Y'))\n",
    "\n",
    "    tweet_id = tweet['id']\n",
    "    tweet_text = tweet['text']\n",
    "    if('extended_tweet' in tweet.keys()):\n",
    "        tweet_text = tweet['extended_tweet']['full_text']\n",
    "\n",
    "    user_id = tweet['user']['id']\n",
    "    followers_count = tweet['user']['followers_count']\n",
    "    friends_count = tweet['user']['friends_count']\n",
    "\n",
    "    user_mentions = tweet['entities']['user_mentions']\n",
    "    screen_names = [user_mention['screen_name'] for user_mention in user_mentions]\n",
    "    screen_name = tweet['user']['screen_name']\n",
    "    retweet_count = tweet['retweet_count']\n",
    "    favorite_count = tweet['favorite_count']\n",
    "    retweeted = tweet['retweeted']\n",
    "    tweet_row = {'date':ts,\n",
    "                 'tweet_id' : tweet_id,\n",
    "                 'user_id' : user_id,\n",
    "                 'followers_count' : followers_count,\n",
    "                 'friends_count' : friends_count,\n",
    "                 'user_mentions' : screen_names,\n",
    "                 'screen_name' : screen_name,\n",
    "                 'retweet_count' : retweet_count,\n",
    "                 'favorite_count' : favorite_count,\n",
    "                 'retweeted' : retweeted,\n",
    "                 'full_text':tweet_text}\n",
    "    return tweet_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ref : https://stackoverflow.com/questions/1060279/iterating-through-a-range-of-dates-in-python\n",
    "def daterange(start_date, end_date):\n",
    "    for n in range(int ((end_date - start_date).days)):\n",
    "        yield start_date + timedelta(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch Swiggy and Zomato Tweets. Store raw data under swiggy/json and zomato/json folder.\n",
    "Processed selected data is stored under zomato and swiggy folders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2> Fetch data for Swiggy </H2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ref : https://medium.com/@poconnell732/acquiring-free-historical-geo-located-data-from-twitter-1f8f2821e9b1\n",
    "api = TwitterAPI(consumer_key=consumer_key,\n",
    "                 consumer_secret=consumer_secret,\n",
    "                 access_token_key=access_token,\n",
    "                 access_token_secret=access_token_secret)\n",
    "# PRODUCT = '30day' #Using 30day API\n",
    "# LABEL = '30daysdev' # sandbox name\n",
    "PRODUCT = 'fullarchive' #use full archive API\n",
    "LABEL = 'devbox1' \n",
    "web_request_count = 0\n",
    "                \n",
    "start_month_date = date(2019, 7, 16)\n",
    "end_month_date = date(2019, 7, 16)\n",
    "\n",
    "#for date in daterange(start_month_date, end_month_date):\n",
    "list_tweets = []\n",
    "next_token = {}\n",
    "date_str = \"201905_02\"\n",
    "start_date = \"201904200000\"\n",
    "end_date = \"201905200000\"\n",
    "print(start_date)\n",
    "print(end_date)\n",
    "\n",
    "while ((next_token is not None) and (web_request_count<2)):\n",
    "    print(next_token)\n",
    "    if not next_token :\n",
    "        req_dict = {'query' : '@swiggy_in OR @swiggyCares lang:en', 'fromDate': start_date, 'toDate': end_date,}\n",
    "    else :\n",
    "        req_dict['next'] = next_token\n",
    "\n",
    "    web_request_count += 1\n",
    "    print('web_request_count: ', web_request_count)\n",
    "    req = api.request('tweets/search/%s/:%s' % (PRODUCT, LABEL), req_dict)\n",
    "    print('status_code: ', req.status_code)\n",
    "    response = req.json()\n",
    "    if('next' in response.keys()):\n",
    "        next_token = response['next']\n",
    "    else :\n",
    "        next_token = None\n",
    "\n",
    "    #print(response)\n",
    "    results = response['results']\n",
    "    with open('data/swiggy/json/' + date_str + '_' + str(web_request_count) + '.json', 'w+') as f:\n",
    "        json.dump(results, f)\n",
    "\n",
    "    #print(results)\n",
    "    for tweet in results:\n",
    "        tweet_row = process_tweet(tweet)\n",
    "        list_tweets.append(tweet_row)\n",
    "    df = pd.DataFrame(list_tweets)\n",
    "df.to_csv('data/swiggy/' + date_str + '.csv', index=False)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2> Fetch data for Zomato </H2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ref : https://medium.com/@poconnell732/acquiring-free-historical-geo-located-data-from-twitter-1f8f2821e9b1\n",
    "from TwitterAPI import TwitterAPI\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import timedelta, date\n",
    "\n",
    "api = TwitterAPI(consumer_key=consumer_key,\n",
    "                 consumer_secret=consumer_secret,\n",
    "                 access_token_key=access_token,\n",
    "                 access_token_secret=access_token_secret)\n",
    "PRODUCT = '30day' #Using 30day API\n",
    "LABEL = '30daysdev' # sandbox name\n",
    "PRODUCT = 'fullarchive'\n",
    "LABEL = 'devbox1' \n",
    "web_request_count = 0\n",
    "                \n",
    "start_month_date = date(2019, 7, 16)\n",
    "end_month_date = date(2019, 7, 16)\n",
    "\n",
    "#for date in daterange(start_month_date, end_month_date):\n",
    "list_tweets = []\n",
    "next_token = {}\n",
    "date_str = \"201806_02\"\n",
    "start_date = \"201805200000\"\n",
    "end_date = \"201806200000\"\n",
    "print(start_date)\n",
    "print(end_date)\n",
    "\n",
    "while (next_token is not None) and (web_request_count<2):\n",
    "    print(next_token)\n",
    "    if not next_token :\n",
    "        req_dict = {'query' : '@zomatocare OR @zomatoIn lang:en', 'fromDate': start_date, 'toDate': end_date}\n",
    "    else :\n",
    "        req_dict['next'] = next_token\n",
    "\n",
    "    web_request_count += 1\n",
    "    print('web_request_count: ', web_request_count)\n",
    "    req = api.request('tweets/search/%s/:%s' % (PRODUCT, LABEL), req_dict)\n",
    "    print('status_code: ', req.status_code)\n",
    "    response = req.json()\n",
    "    if('next' in response.keys()):\n",
    "        next_token = response['next']\n",
    "    else :\n",
    "        next_token = None\n",
    "\n",
    "    print(response)\n",
    "    results = response['results']\n",
    "    with open('data/zomato/json/' + date_str + '_' + str(web_request_count) + '.json', 'w+') as f:\n",
    "        json.dump(results, f)\n",
    "\n",
    "    #print(results)\n",
    "    for tweet in results:\n",
    "        tweet_row = process_tweet(tweet)\n",
    "        list_tweets.append(tweet_row)\n",
    "    df = pd.DataFrame(list_tweets)\n",
    "df.to_csv('data/zomato/' + date_str + '.csv', index=False)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
