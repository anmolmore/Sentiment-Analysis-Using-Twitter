---
title: "Sentiment Analysis"
output:
  html_document: default
  html_notebook: default
---

Let me first read in a common dataset for all our sentiment-an examples.

```{r}
# IBM Q3 2016 analyst call transcript
x = readLines('zomato.txt')
```


### Using qdap for sentiment analysis

qdap or the qualitative discourse analysis package offers a relatively rigorous sentiment-An approach. However, it is limited to a standard dictionary (from Princeton).

Our main use of sentiment-An in this session shall be with tidytext library. However, for completeness sake, let's quickly do qdap first.

```{r eval=FALSE}
# loading required libraries
require(qdap) || install.packages("qdap") # ensure java is up to date!
library(qdap)
```


```{r}
require(qdap)
```

Use `polarity()` func from qdap and store its output into various variables. Try `?qdap::polarity` for more detail.

```{r}
# apply polarity() func from qdap to compute sentiment polarity
t1 = Sys.time()   # set timer
  pol = qdap::polarity(x)         # Calculate the polarity from qdap dictionary
  wc = pol$all[,2]                  # Word Count in each doc
  val = pol$all[,3]                 # average polarity score
  p  = pol$all[,4]                  # Positive words info
  n  = pol$all[,5]                  # Negative Words info  
Sys.time() - t1  # how much time did the above take?


```
View output variables.

```{r}
head(pol$all)
```

Summarize polarity scores for the corpus

```{r}
head(pol$group)
```

What positive words were there in the corpus?

```{r}
positive_words = unique(setdiff(unlist(p),"-"))  # Positive words list, do ?dplyr::setdiff
print(positive_words)       # Print all the positive words found in the corpus
```

We can use the wordlcoud or barchart or other display aids as required on the list above.

What negative words were there in the corpus?

```{r}
negative_words = unique(setdiff(unlist(n),"-"))  # Negative words list
print(negative_words)       # Print all neg words
```

While qdap is nice and easy, its limitations are also there. 

E.g., it uses an inbuilt wordlist that cannot be customized for particular contexts. 

Another is that it does exact matching and misses minor variations in sentiment words. It suffers from the old problems of *synonymy* (i.e., multiple words that have similar meanings) and *polysemy* (words that have more than one meaning). 

While these are old standing problems and cannot be eliminated in the bag of words approach, more flexibility would enable a substantial mitigation at least.

### Sentiment-An with Tidytext

There are 3 inbuilt sentiment dictionaries as of now in tidytext with a fourth under development. 

To briefly see what these are, return to the slides.

The nice thing about tidytext is that while there are 3 inbuilt sentiment dictionaires in different output formats for convenience, we can also create and customize our own wordlists as needed. 

```{r}
require(tidytext)
require(tidyr)
require(dplyr)
```

The tidytext package contains all three sentiment lexicons in the sentiments dataset.
```{r}
sentiments    # over 27k words sentiment-ized and scored across the 3 lexicons
```

```{r}
#sentiments %>% 
#          filter(lexicon == "AFINN") %>% 
#          head()
```


Let's start simple, with Bing.

```{r}
textdf = data_frame(text = x)   # convert to data frame

bing = get_sentiments("bing")   # put all of the bing sentiment dict into object 'bing'
bing     # view bing object
```

Which docs are most positive and negative in the corpus?

```{r}
senti.bing = textdf %>%
      mutate(linenumber = row_number()) %>%   # build line num variable
      ungroup() %>%
      unnest_tokens(word, text) %>%
      inner_join(get_sentiments("bing")) %>%
      count(sentiment, index = linenumber %/% 1, sort = FALSE) %>%
      mutate(method = "bing")    # creates a column with method name

senti.bing
```

Now let's see the distribution of positive and negative sentiment within documents across the corpus.

Note use of the `spread()` function to combine extra row pertaining to some index (doc) and make an extra column. Do `?tidyr::spread` for more info.

```{r}
bing_df = data.frame(senti.bing %>% spread(sentiment, n, fill = 0))

head(bing_df)
```

Can we combine the negative and positive rows, by say, subtracting negative from poisitive score and thereby computing some polarity score for each line? Yes, see code below.

```{r}
bing_pol = bing_df %>% 
  mutate(polarity = (positive - negative)) %>%   #create variable polarity = pos - neg
  arrange(desc(polarity), index)    # sort by polarity

bing_pol %>%  head()
```

Now for some quick visualization of the distribution of sentiment across the analyst call. See code below.

```{r}
require(ggplot2)
# plotting running sentiment distribution across the analyst call
ggplot(bing_pol, 
       aes(index, polarity)) +
       geom_bar(stat = "identity", show.legend = FALSE) +
      labs(title = "Sentiment in IBM analyst call corpus",
             x = "doc",  
             y = "Sentiment")
        

```

Another quick visualization. We want to see which words contributed most to positive or neg sentiment in the corpus using the bing lexicon.

So first we create a count of bing sentiment words that occur a lot in the corpus. 

```{r}

bing_word_counts <- textdf %>%
                    unnest_tokens(word, text) %>%
                    inner_join(bing) %>%
                    count(word, sentiment, sort = TRUE) %>%
                    ungroup()

bing_word_counts

```

Now `ggplot` it and see.

```{r}
bing_word_counts %>%
  filter(n > 50) %>%
  mutate(n = ifelse(sentiment == "negative", -n, n)) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  ylab("Contribution to sentiment")
```

And then there're wordclouds.

```{r warnings=FALSE}
require(wordcloud)

# build wordcloud of commonest tokens
textdf %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))
```


### Sentiment-An with AFINN

```{r}
# get AFINN first
AFINN <- get_sentiments("afinn")
AFINN

# inner join AFINN words and scores with text tokens from corpus
senti.afinn = textdf %>%
      mutate(linenumber = row_number()) %>%
      ungroup() %>%
      unnest_tokens(word, text) %>%
      inner_join(AFINN) %>%    # returns only intersection of wordlists and all columns
      group_by(index = linenumber %/% 1) %>% 
      summarise(sentiment = sum(score)) %>% 
      mutate(method = "afinn")

senti.afinn
```

```{r}
data.frame(senti.afinn) %>% head()
```

Can we combine sentiment-An with bigrams?

E.g., Suppose you want to list bigrams where first word is a sentiment word. See code below.

```{r}
# first construct and split bigrams into word1 and word2
ibm_bigrams_separated <- textdf %>%
                unnest_tokens(bigram, text, 
                token = "ngrams", n = 2) %>%
                separate(bigram, c("word1", "word2"), sep = " ")

ibm_bigrams_separated
```

Next, inner join with AFINN

```{r}
# examine the most frequent bigrams whose first word is a sentiment word
senti_bigrams <- ibm_bigrams_separated %>%
              inner_join(AFINN, by = c(word1 = "word")) %>%     # word1 is from bigrams and word from AFINN
              ungroup()
senti_bigrams
```

Filter out stopwords and then do inner join with AFINN.

```{r}
# what if we want sentiment associated with proper words, not stopwords?
senti_bigrams_filtered = ibm_bigrams_separated %>%
                        filter(!word1 %in% stop_words$word) %>%
                        filter(!word2 %in% stop_words$word) %>%
                        inner_join(AFINN, by = c(word1 = "word")) %>%     # word1 is from bigrams and word from AFINN
                        ungroup()

senti_bigrams_filtered
```

And so on.

Heading finally to the NRC dictionary.

### Sentiment-An with the NRC dictionary

```{r}
# view nrc dict structure
nrc = get_sentiments("nrc")
nrc

# 
senti.nrc = textdf %>%
      mutate(linenumber = row_number()) %>%
      ungroup() %>%
      unnest_tokens(word, text) %>%
      inner_join(get_sentiments("nrc")) %>%
      count(sentiment, index = linenumber %/% 1, sort = FALSE) %>%  # %/% gives quotient
      mutate(method = "nrc")

senti.nrc %>% head()
```

```{r}
# make a neat table out of the 8 emotion dimensions
a = data.frame(senti.nrc %>% spread(sentiment, n, fill = 0))
head(a)
```

Suppose you want to see what joyful words most occurred in the corpus. 

```{r}

ibm_joy = textdf %>%
      unnest_tokens(word, text) %>%
      inner_join(nrc) %>%
      filter(sentiment == "joy") %>%
      count(word, sort = TRUE)

ibm_joy %>% head()

```

